{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoO4yJw03ZbvlXhVcbUpL8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAHMOUDPD/Optimization-algorithms/blob/main/Some_constraint_unconstrained-Algorithms_\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKQ3pFtOxXIa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# ......"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$Question$: (Broad statement of the question). State Unconstrained and Constrained Algorithms taught in Introductory Course to Optimization, MTH 640. Express their advantages and disadvantages. Moreover, I give their Python implementation with simple examples to show their outputs.\n",
        "\n",
        "###University: KMUTT\n",
        "###Course instructor: Prof. Parin Chaipunya\n",
        "\n",
        "###The course material can be obtain via: https://parinchaipunya.com/wp-content/uploads/2023/01/optim_lect_rupp-1.pdf"
      ],
      "metadata": {
        "id": "7JSyInHvycCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.0 Unconstrained Optimization-Algorithms\n",
        "###1.1 Gradient Descent, GD (or commonly called steepest descent in some communities)\n",
        "\n",
        "Suppose that $f:\\mathbb{R^n} \\to \\mathbb{R},$ the general outlined of the scheme is as follows:\n",
        "\n",
        "$Algorithm ~1.1~ GD$ \n",
        "\n",
        "**---------------------------------------------------------**\n",
        "\n",
        "**Initialization:**\n",
        "\n",
        "$ \\quad Pick~ a~ start~ point~x^0 \\in \\mathbb{R^n}. $\n",
        "\n",
        "$ \\quad Set~ k ← 0.$\n",
        "\n",
        "**While:** $∇f(x^k) \\neq 0;$\n",
        "\n",
        "$\\quad Choose ~t_k > 0(using~line~search~approaches)~and~ d^k \\in \\mathbb{R^n}. $\n",
        "\n",
        "$\\quad Compute~ x^{k+1} ← x^k + t^k d^k .$\n",
        "\n",
        "$\\quad Update~k ← k+1.$\n",
        "\n",
        "**---------------------------------------------------------**\n",
        "\n"
      ],
      "metadata": {
        "id": "Sud9HWN-x7k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GD( f, grad_f, x0, tol=1e-6, max_iter=1000  ):\n",
        "    \"\"\"\n",
        "    f: function to minimize\n",
        "    grad_f: gradient of f\n",
        "    A: n x n positive definite matrix\n",
        "    b: n-dimensional vector\n",
        "    x0: initial guess for solution\n",
        "    tol: tolerance for stopping criteria\n",
        "    max_iter: maximum number of iterations\n",
        "    \n",
        "    Returns:\n",
        "    - x_bar: optimal solution\n",
        "    - f_vals: a list of function values at each iteration.\n",
        "    \"\"\"\n",
        "    k = 0\n",
        "    x = x0\n",
        "    fvals = [f(x)]\n",
        "    grad0 = grad_f(x)\n",
        "    d0 = -grad0\n",
        "    \n",
        "    while np.linalg.norm(d0) > tol and k < max_iter:\n",
        "        grad = grad_f(x)\n",
        "        # compute alpha using some line search e.g armijo or wolfe\n",
        "        tk = armijo_ls( f, x, grad_f, d0 )\n",
        "        \n",
        "        dk = -grad\n",
        "        xk = x + tk * dk\n",
        "        # update the terms\n",
        "        d0 = dk\n",
        "        x = xk\n",
        "        fvals.append(f(x))\n",
        "        k = k + 1\n",
        "    return x, fvals"
      ],
      "metadata": {
        "id": "7fowOXvRyPYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "   return np.sum((x - np.array([1, 2])) ** 2)\n",
        "\n",
        "def grad_f(x):\n",
        "   return 2 * (x - np.array([1, 2]))\n",
        "\n",
        "x0 = [10,10]\n",
        "\n",
        "x_opt = GD( f, grad_f, x0)\n",
        "\n",
        "# Print the solution\n",
        "print(f\"The optimal solution is:\\n{x_opt[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX_ufJZFxTZ8",
        "outputId": "95aa7210-e2af-41df-ee64-444b528dc1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal solution is:\n",
            "[1. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def armijo_ls( f, xk, grad_f, dk, alpha=1, rho=0.5, c=1e-4 ):\n",
        "    \"\"\"\n",
        "    Perform Armijo line search to find a step size that satisfies the Armijo condition.\n",
        "    \n",
        "    Parameters:\n",
        "    f : The objective function to be minimized.\n",
        "    xk : The current point.\n",
        "    dk : The search direction.\n",
        "    alpha : The initial step size.\n",
        "    rho : The reduction factor for the step size.\n",
        "    c : The Armijo constant.\n",
        "    \n",
        "    Returns:\n",
        "    alpha: The step size that satisfies the Armijo condition.\n",
        "    \"\"\"\n",
        "    fk = f(xk)\n",
        "    gk = grad_f(xk)\n",
        "    \n",
        "    psi = np.dot(gk, dk)\n",
        "    \n",
        "    while f(xk + alpha * dk) > fk + c * alpha * psi:\n",
        "        alpha = alpha * rho\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "7FCgBz62xQQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advantages of GD:\n",
        "\n",
        "\n",
        "*   GD is guaranteed to converge to a local minimum of a convex function. Moreover, if the function is strongly convex, it converges at a linear rate. On the other hand, if the function is non-convex, GD is not guaranteed to converge to the global minimum; however, it may converge to a good solution if the function landscape is well-behaved and the method is well-tuned.\n",
        "*   GD is simple to implement, can easily be parallelized, and can efficiently be used for large-scale problems.\n",
        "*   GD can easily be adapted to handle non-smooth optimization problems using appropriate modifications such as subgradient and proximal gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Dis-advantages of GD:\n",
        "\n",
        "\n",
        "*   The performance of GD can be heavily dependent on the initial guess of the parameters. A poor initialization can lead to slow convergence or even convergence to a local minimum far from the global minimum.\n",
        "*   GD may produce a zig-zag pattern between steps. \n",
        "*   The choice of $step~size,t_k$ (or often called learning rate in some research communities) plays a vital role in the algorithm's convergence rate. If the $t_k$ is too large, the algorithm may not converge. On the other hand, if the $t_k$ is too small, that can lead to very slow convergence.\n",
        "* It is computationally expensive in some scenarios to the gradient of the objective function; this is mainly the case when a large-scale dataset defines the problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "g-gRybxkzLOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2 (Linear) Conjugate Gradient Descent, CG \n",
        "Here linear CG can be understood as an algorithm formulated for solving a linear system given as: \n",
        "\\begin{equation}\n",
        "Ax = b\n",
        "\\end{equation}\n",
        "where $A\\in \\mathbb{R^{n\\times n}} $ is a symmetric positive definite matrix and $b\\in \\mathbb{R^n}.$\n",
        "\n",
        "$Algorithm ~1.2~ LCG$ \n",
        "\n",
        "**---------------------------------------------------------**\n",
        "\n",
        "**Initialization:**\n",
        "\n",
        "$ \\quad Pick~ a~ start~ point~x^0 \\in \\mathbb{R^n}. $\n",
        "\n",
        "$ \\quad Set~ r^0  ← Ax^0 - b,~d^0 ← - r^0,~ k ← 0.$\n",
        "\n",
        "**While:** $r^k \\neq 0;$\n",
        "\n",
        "$\\quad t_k ← -\\frac{(r^k)^{T} d^k}{(d^k)^{T}Ad^{k}}$\n",
        "\n",
        "$\\quad x^{k+1} ← x^k + t^k d^k .$\n",
        "\n",
        "$\\quad r^{k+1} ← Ax^{k+1} - b .$\n",
        "\n",
        "$\\quad \\beta_{k+1} ← \\frac{(r^{k+1})^{T}Ad^k}{(d^k)^{T}Ad^{k}}$\n",
        "\n",
        "$\\quad d^{k+1} ← -r^{k+1} + \\beta_{k+1} d^k.$\n",
        "\n",
        "$\\quad Update~k ← k+1.$\n",
        "\n",
        "**---------------------------------------------------------**\n"
      ],
      "metadata": {
        "id": "GTX7-FYq59sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LCG(A, b, x0=None, tol=1e-6, max_iter=1000):\n",
        "    \"\"\"\n",
        "    f: function to minimize f(x) =  A x - b\n",
        "    grad_f: gradient of f\n",
        "    A: n x n positive definite matrix\n",
        "    b: n-dimensional vector\n",
        "    x0: initial guess for solution\n",
        "    tol: tolerance for stopping criteria\n",
        "    max_iter: maximum number of iterations\n",
        "    \n",
        "    Returns:\n",
        "    x_bar: optimal solution\n",
        "    \"\"\"\n",
        "    if x0 is None:\n",
        "        x0 = np.zeros_like(b)\n",
        "    \n",
        "    r0 = A.dot(x0) - b\n",
        "    d0 = -r0\n",
        "    x = x0\n",
        "    k = 0\n",
        "    while np.linalg.norm(r0) > tol and k < max_iter:\n",
        "        \n",
        "        Ad = A.dot(d0)\n",
        "        # Evalute the step-size, tk\n",
        "        tk = - np.dot(r0, d0) / np.dot(d0, Ad)\n",
        "        # Update for next point\n",
        "        x = x0 + tk * d0\n",
        "        r_new = A.dot(x) - b\n",
        "        # Compute the beta parameter\n",
        "        beta = np.dot(r_new, Ad) / np.dot(d0, Ad)\n",
        "        d_new = -r_new + beta * d0\n",
        "        \n",
        "        # Update terms\n",
        "        r0 = r_new\n",
        "        x0 = x\n",
        "        d0 = d_new\n",
        "        k = k + 1\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "rKJNxJQFB5fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[-2,1], [1, 1]])\n",
        "b = np.array([1, 1])\n",
        "\n",
        "x_opt = LCG(A, b, x0=None)\n",
        "\n",
        "# Print the solution\n",
        "print(f\"The optimal solution is:\\n{x_opt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLBkCiJlx67z",
        "outputId": "237c919e-227e-4896-ef2f-0f15beb32b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal solution is:\n",
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advantages of LCG:\n",
        "\n",
        "\n",
        "*   The zig-zigging pattern of GD's steps is mitigated by adding a weight parameter $\\beta^{k+1}$ in the search direction, $d^{k+1}$ such that the direction is $A$-conjugate to $d^k$, i.e. $d^{(k+1)T}Ad^k = 0$.\n",
        "*   The LCG is simple to implement and requires low memory requirement; as such, it is particularly suitable for large-scale problems.\n",
        "*  The method converges to the solution within $n$ steps for quadratic problems.\n",
        "\n",
        "\n",
        "###Dis-advantages of LCG:\n",
        "\n",
        "\n",
        "* The CG algorithm is mostly sensitive to the choice of the initial guess and the choice of the preconditioner.\n",
        "* The CG method also requires good step size initialisation, $\\alpha.$\n",
        "* It often behaves differently in practice from theory.\n"
      ],
      "metadata": {
        "id": "m9zar9m2PCzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.0 Constrained Optimization-Algorithms\n",
        "\n",
        "###2.1 Projected Gradient Descent, PGD \n",
        "\n",
        "The PGD is a modified GD extension using a projection operator that forces the infeasible update onto the constraint set.\n",
        "\n",
        "***Definition 1.0:***\n",
        "\n",
        "Suppose $C ⊂ \\mathbb{R^n}$ be nonempty, closed, and convex. Then the *metric projection over the set $C$* is the map $P_C : \\mathbb{R^n} \\to C$ given by\n",
        "\n",
        "\\begin{equation}\n",
        "P_C(x) := argmin_{u\\in C} \\frac{1}{2}\\|u - x \\|^2\n",
        "\\end{equation}\n",
        "for all $x\\in \\mathbb{R^n}.$\n",
        "\n",
        "\n",
        "$Algorithm ~2.1~ PGD$ \n",
        "\n",
        "**----------------------------------------------------------------------------------------------------**\n",
        "\n",
        "**Initialization:**\n",
        "\n",
        "$ \\quad Pick~ a~ start~ point~x^0 \\in \\mathbb{R^n} $ and an unconstrained descent step length $σ > 0.$\n",
        "\n",
        "$ \\quad Set~ k ← 0.$\n",
        "\n",
        "**While:** $k = 0$ or $\\|d^k\\| \\neq 0;$\n",
        "\n",
        "$\\quad y^k ← P_C (x^k - σ ∇f(x^k)).$\n",
        "\n",
        "$\\quad d^k ← y^k - x^k.$\n",
        "\n",
        "$\\quad Determine~ the~ step-size~ according~ to~ the~ Armijo~ line-search~ t_{k} = β^i ~(i.e~ s=0),$\n",
        "\n",
        "$\\quad where~i\\in \\mathbb{N} ∪ \\{0\\}~is~the~smallest~integer~satisfying~$\n",
        "\\begin{equation}\n",
        "f(x^k + sβ^i d^k ) \\leq f(x^k) + \\alpha s β^i ∇f(x^k)^{T}d^k\n",
        "\\end{equation}\n",
        "\n",
        "$\\quad Compute~ x^{k+1} ← x^k + t^k d^k .$\n",
        "\n",
        "$\\quad Update~k ← k+1.$\n",
        "\n",
        "**--------------------------------------------------------------------------------------------------------**\n"
      ],
      "metadata": {
        "id": "ULza3HmSCOBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ProjGD(f, grad_f, C, x0, tol=1e-6, max_iter=1000):\n",
        "    \"\"\"\n",
        "    Projected gradient descent algorithm for optimizing a function f over the feasible set C.\n",
        "    Inputs:\n",
        "    - f: a function that takes a vector x as input and returns a scalar.\n",
        "    - grad_f: a function that takes a vector x as input and returns the gradient of f at x.\n",
        "    - C: a tuple (C_min, C_max) that defines the feasible set as the set of all vectors x\n",
        "         such that C_min <= x <= C_max (element-wise).\n",
        "    - x0: the initial point for the algorithm.\n",
        "    - tol: the tolerance for the stopping criterion.\n",
        "    - max_iter: the maximum number of iterations.\n",
        "    \n",
        "    Returns:\n",
        "    - x: the final iterate.\n",
        "    - f_vals: a list of function values at each iteration.\n",
        "    \"\"\"\n",
        "    k = 0\n",
        "    x = x0\n",
        "    fvals = [f(x)]\n",
        "    grad0 = grad_f(x)\n",
        "    d0 = -grad0\n",
        "    \n",
        "    while np.linalg.norm(d0) > tol and k < max_iter:\n",
        "        grad = grad_f(x)\n",
        "        # compute alpha using some line search e.g armijo or wolfe\n",
        "        tk = armijo_ls( f, x, grad_f, d0 )\n",
        "        yk = ProjC((x - tk * grad), C)\n",
        "        \n",
        "        dk = yk - x\n",
        "        \n",
        "        xk = x + tk * dk\n",
        "        # update the terms\n",
        "        d0 = dk\n",
        "        x = xk\n",
        "        fvals.append(f(x))\n",
        "        k = k + 1\n",
        "    return x, fvals\n"
      ],
      "metadata": {
        "id": "zkGmmICTKQZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ProjC(x, C):\n",
        "    \"\"\"\n",
        "    Project the vector x onto the feasible set C.\n",
        "    \"\"\"\n",
        "    return np.clip(x, C[0], C[1])"
      ],
      "metadata": {
        "id": "nAqjG-yiyUVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "   return np.sum((x - np.array([1, 2])) ** 2)\n",
        "\n",
        "def grad_f(x):\n",
        "   return 2 * (x - np.array([1, 2]))\n",
        "    \n",
        "C = (np.array([0, 0]), np.array([2, 3]))\n",
        "x0 = np.array([10, -10])\n",
        "\n",
        "x, fvals = ProjGD(f, grad_f, C, x0)\n",
        "\n",
        "\n",
        "# Print the solution\n",
        "print(f\"The optimal solution is:\\n{x}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06BouxiyYzS",
        "outputId": "27215322-6161-4c1b-86cd-172670968abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal solution is:\n",
            "[1.00000027 1.99999964]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advantages of PGD:\n",
        "\n",
        "\n",
        "* The PCG method extends the capabilities of GD to constraint optimization cases. \n",
        "* One of the advantages of PCG includes simplicity in implementation (especially for problems with simple\n",
        "bound constraints),\n",
        "* Also, it efficiently exploits the separable structure of the underline mapping or its constraints, and the method uses little storage.\n",
        "\n",
        "### Disadvantages of PGD:\n",
        "\n",
        "* It requires an appropriate choice of step length, which is expensive to do in practice.\n",
        "* The projection operator is computationally expensive when the feasible region is a non-convex constraint set or when the constraints are highly non-linear.\n"
      ],
      "metadata": {
        "id": "jSd5n84QIppx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 Uzawa Algorithm\n",
        "\n",
        "Uzawa's algorithm is simply the PGD applied to the $dual~problem$\n",
        "The dual problem of constrained optimization (ref. $Opt(f,g)$ in which $Opt(f,g)$ ) is known as $primal-problem$ is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "max_{μ\\in \\mathbb{R^{r}_+}} q(μ), \\quad \\quad q(μ) :=inf_{v\\in \\mathbb{R^{n}}} L(v, μ).\n",
        "\\end{equation}\n",
        "\n",
        "where $L(v, μ):= f(v) + ∑_{i=1}^r μ_{i}g_{i}(v).$ The idea of Uzawa's algorithm is that to each dual variable $μ \\in \\mathbb{R^{r}_{+}},$ the parametrized unconstrained optimization, $Opt(L(⋅, μ))$ has a solution $u_μ \\in \\mathbb{R^n}.$\n",
        "\n",
        "Then at each iteration, i.e. $for~k = 0,1,2,⋯$, the update on the dual variable, say, $λ^k$ (computed from projected steepest ascent) yields the primal variable $u^k \\in Opt(L(⋅, λ^k)).$ Thus, as $k\\to +∞,$ we wish to $(u^k, λ^k)$ would converge to a saddle point of $L,$ or at least $u^k$ would converge to a solution of $Opt(f,g)$.\n",
        "\n",
        "Uzawa's algorithm has an explicit formula for evaluating the projection $P_{+}:=P_{\\mathbb{R^{r}_{+}}}:\\mathbb{R^{r}} \\to \\mathbb{R^{r}_{+}}$ and the gradient $∇q$ of the dual objective function, $q$  when $u_{μ}$ is available for each $μ.$ This has the following coordinate-wise expression as follows:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "P_{+}(ϑ_{i}) := max\\{ϑ_{i}, 0\\} \\quad ∇ q(μ)_{i} = g_{i}(u_μ)\n",
        "\\end{equation}\n",
        "for each $i=1,2,⋯, r.$\n",
        "\n",
        "$Algorithm ~2.2~ Uzawa$ \n",
        "\n",
        "\n",
        "**-----------------------------------------------------------------------------**\n",
        "\n",
        "**Initialization:**\n",
        "\n",
        "$ \\quad Pick~ a~ start~ point~x^0 \\in \\mathbb{R^n} $ and an unconstrained descent step length $σ > 0.$\n",
        "\n",
        "$ \\quad Set~ k ← 0.$\n",
        "\n",
        "**While:** $\\|d^k\\| \\neq 0;$\n",
        "\n",
        "$\\quad u^k: ~Chosen~ from~argmin_{v\\in \\mathbb{R^{n}}} L(⋅, λ^k).$\n",
        "\n",
        "$\\quad λ^{k+1} ← P_+ (λ^k + σ ∇q(λ^k)),$\n",
        "\n",
        "$\\quad \\quad i.e~λ^{k+1}_{i} = max\\{0, λ^{k}_{i} + σ g_{i}(u^k) \\}~for~i=1,2,⋯,r.$\n",
        "\n",
        "$\\quad Update~k ← k+1.$\n",
        "\n",
        "**-----------------------------------------------------------------------------**\n"
      ],
      "metadata": {
        "id": "6Whz21zPLC2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import optimize\n",
        "\n",
        "def uzawa_projection(Q, c, A, b, lambda0, sigma, tol, max_iter):\n",
        "    \"\"\"\n",
        "    Solves the linearly constrained optimization problem\n",
        "    min 1/2 * x^T Q x - b^T x\n",
        "    s.t. g(x) = Ax + b\n",
        "\n",
        "    using Uzawa's algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        Q: n x n positive definite matrix\n",
        "        c: n-dimensional vector\n",
        "        \n",
        "        A: m x n matrix\n",
        "        b: m-dimensional vector\n",
        "        lambda0: initial Lagrange multiplier\n",
        "        sigma: unconstrained descent step length\n",
        "        tol: tolerance for stopping criteria\n",
        "        max_iter: maximum number of iterations\n",
        "\n",
        "    Returns:\n",
        "        x_k (ndarray): the optimal solution\n",
        "    \"\"\"\n",
        "    u0 = np.zeros((Q.shape[0], 1))  # initial guess\n",
        "    k = 0\n",
        "\n",
        "    while k < max_iter:\n",
        "        # Compute u_k   np.dot(lambda_k, A.dot(u) - b)\n",
        "        L = lambda x: (0.5 * x.T @ Q @ x - c.dot(x) + lambda0.T@(A.dot(x) - b))\n",
        "        \n",
        "        u_k = optimize.minimize(L, x0 = u0, method='BFGS').x # we can use CG method as well\n",
        "\n",
        "        # Compute lambda_k\n",
        "        lambda_k = np.maximum(0, lambda0 + sigma * ( A.dot(u_k) - b)) # maybe we can use Line Search strategy to find sigma.        \n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(u_k - u0) < tol and np.linalg.norm(lambda_k - lambda0) < tol:\n",
        "           break\n",
        "        u0 = u_k\n",
        "        lambda0 = lambda_k\n",
        "        k += 1\n",
        "\n",
        "    return u_k, lambda_k, k"
      ],
      "metadata": {
        "id": "RGLRjsGdlthX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the problem\n",
        "Q = np.array([[2, 1], [1, 2]])\n",
        "c = np.array([-2, -3])\n",
        "\n",
        "\n",
        "A = np.array([[-2,1], [1, 1]])\n",
        "b = np.array([1, 1])\n",
        "# P = lambda x: np.array([[x[0] + x[1]], [x[0]**2 + x[1]**2 - 1]])  # unit circle and x1 + x2 = 0\n",
        "lambda0 = np.array([0, 0])  # initial guess\n",
        "sigma = .5\n",
        "tol = 1E-5\n",
        "max_iter = 100\n",
        "\n",
        "# Solve using Uzawa's algorithm\n",
        "x_opt = uzawa_projection(Q, c, A, b, lambda0, sigma, tol, max_iter)\n",
        "\n",
        "# Print the solution\n",
        "print(f\"The optimal solution is:\\n{x_opt[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR8NMFqVyups",
        "outputId": "777958be-4f5c-4199-a506-1978d18335cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal solution is:\n",
            "[-0.33333324 -1.33333314]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-43c09df9b534>:32: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
            "  u_k = optimize.minimize(L, x0 = u0, method='BFGS').x # we can use CG method as well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advantages:\n",
        "\n",
        "* The method is simple to implement and can handle linear constraints efficiently.\n",
        "* It can be guaranteed to converge to a feasible solution if the constraints are consistent and the optimization problem is bounded.\n",
        "* Also, the method has an explicit form for computing the projection operator\n",
        "\n",
        "\n",
        "\n",
        "###Dis advantages:\n",
        "\n",
        "* The method may need a better convergence rate, especially for poorly conditioned problems.\n",
        "* It can be sensitive to the choice of penalty parameter $sigma$, and thus, it may require careful tuning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bOBjb-3ITOz5"
      }
    }
  ]
}